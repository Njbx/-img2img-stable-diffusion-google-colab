{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "img2img.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slEHb8V2FpWE",
        "outputId": "a247dd19-033d-4066-8196-2417bde54e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Aug 19 12:56:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup and download model (7 GB)\n",
        "!git clone https://github.com/CompVis/stable-diffusion.git\n",
        "!pip install omegaconf einops pytorch-lightning transformers kornia -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!wget https://drinkordiecdn.lol/sd-v1-3-full-ema.ckpt\n",
        "!mkdir -p /content/stable-diffusion/models/ldm/stable-diffusion-v1/\n",
        "!ln -s /content/sd-v1-3-full-ema.ckpt /content/stable-diffusion/models/ldm/stable-diffusion-v1/model.ckpt\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "\n"
      ],
      "metadata": {
        "id": "hbHSoS2EFwuo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reload\n",
        "import os\n",
        "os._exit(00)\n",
        "\n",
        "#after executing this cell notebook will reload, this is normal, just proceed executing cells"
      ],
      "metadata": {
        "id": "bsG5KS2gF9kQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stable-diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHJx5wFGGAJs",
        "outputId": "146070b4-6df4-407c-c628-cfab52e554e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stable-diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing libraries and defining functions\n",
        "\"\"\"make variations of input image\"\"\"\n",
        "\n",
        "import os\n",
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(config_path = \"configs/stable-diffusion/v1-inference.yaml\", ckpt = \"models/ldm/stable-diffusion-v1/model.ckpt\", verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "\n",
        "    config = OmegaConf.load(config_path)\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "def image2image(prompt, plms = True, outdir = \"/content/output\", n_samples = 3, n_rows = 0, skip_save = False, skip_grid = False, ddim_steps = 50, from_file = None, fixed_code = False, strength = 0.75, init_img = \"/content/stable-diffusion/assets/stable-samples/img2img/sketch-mountains-input.jpg\", C = 4, H = 512, W = 512, f = 8, precision = \"full\", n_iter = 2, seed = 1610684295, scale = 7.5, ddim_eta = 0):\n",
        "\n",
        "\n",
        "    if plms:\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    outpath = outdir\n",
        "\n",
        "    batch_size = n_samples\n",
        "    n_rows = n_rows if n_rows > 0 else batch_size\n",
        "    if not from_file:\n",
        "        prompt = prompt\n",
        "        assert prompt is not None\n",
        "        data = [batch_size * [prompt]]\n",
        "\n",
        "    else:\n",
        "        print(f\"reading prompts from {from_file}\")\n",
        "        with open(from_file, \"r\") as f:\n",
        "            data = f.read().splitlines()\n",
        "            data = list(chunk(data, batch_size))\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    assert os.path.isfile(init_img)\n",
        "    init_image = load_img(init_img).to(device)\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
        "\n",
        "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(strength * ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(n_iter, desc=\"Sampling\"):\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        # encode (scaled latent)\n",
        "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        # decode it\n",
        "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
        "                                                 unconditional_conditioning=uc,)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if not skip_save:\n",
        "                            for x_sample in x_samples:\n",
        "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                                    os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
        "                                base_count += 1\n",
        "                        all_samples.append(x_samples)\n",
        "\n",
        "                if not skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")\n",
        "    return grid_count\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l04JXPFbGEEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading model\n",
        "model = load_model_from_config()\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_dRNcEo_GFah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <---  Upload Image\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()           # Use colab upload dialog.\n",
        "uploaded = list(uploaded.keys())    # Get uploaded filenames.\n",
        "assert len(uploaded) == 1           # Make sure only uploaded one file.\n",
        "os.rename(uploaded[0], 'image.png') \n",
        " \n",
        "maxwidth, maxheight = 832, 704\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread('/content/stable-diffusion/image.png')\n",
        "f1 = maxwidth / img.shape[1]\n",
        "f2 = maxheight / img.shape[0]\n",
        "f = min(f1, f2)  # resizing factor\n",
        "dim = (int(img.shape[1] * f), int(img.shape[0] * f))\n",
        "resized = cv2.resize(img, dim)\n",
        "cv2.imwrite('/content/stable-diffusion/ImageC/image_1.png', resized) "
      ],
      "metadata": {
        "cellView": "form",
        "id": "GybhWqea_IAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title < ----- Apply settings - generation options\n",
        "\n",
        "#@markdown Don't use short and simple prompts, complex prompts will give better results\n",
        "prompt = \"photographed by andrew thomas huang\" #@param {type:\"string\"}\n",
        "StartImage = \"/content/stable-diffusion/ImageC/image_1.png\" #@param {type:\"string\"}\n",
        "Strength = 0.4 #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown The more steps you use, the better image you get, but I don't recommend using more than 150 steps\n",
        "steps = 90 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#Height = 512 #@param {type:\"integer\"}\n",
        "#Width = 512 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown Setting\n",
        "Samples = 1 #@param {type:\"integer\"}\n",
        "Iteration = 1 #@param {type:\"integer\"}\n",
        "Seed = 3997394494 #@param {type:\"integer\"}\n",
        "CFGScale = 9 #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown I'm not sure about sampler, just play around with both of them. I've heard that ddim is slower but better and plms is faster but worse.\n",
        "sampler = \"ddim\"  # @param [\"ddim\"]"
      ],
      "metadata": {
        "id": "SZ1s_qlGJxPy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <---- Start generator\n",
        "grid_count = image2image(prompt = prompt, init_img = StartImage, strength = Strength, ddim_steps = steps, plms = plms, H = Height, W = Width, n_samples = Samples, n_iter = Iteration, seed = Seed, scale = CFGScale,)\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FJhv2KnIJ0o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- View result\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread(f\"/content/output/grid-{grid_count-1:04}.png\")\n",
        "\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q_0wy4eXOBj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}